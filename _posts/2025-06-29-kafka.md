---
layout: post
title: "Kafka"
subtitle: ""
tags: ["system-design"]
readtime: true
---

In the previous post, we looked at what asynchronous messaging is, why it‚Äôs useful, and how message brokers help decouple services so they can communicate without waiting on each other.

Now, let‚Äôs consider a real-world example. Imagine you‚Äôre building a video ingestion service. When a creator uploads a new video, your system needs to:
1. Index it in a search database, so users can discover it.
2. Update an analytics database, tracking how many videos and total minutes the creator has published.

At first glance, using a message broker like RabbitMQ seems like a good fit. You might design it like this: 

1. A topic exchange receives video upload events.
2. Two queues are bound to that exchange: one for the search service, and one for analytics.
3. Each queue has its own consumer service listening for messages.

But this approach has problems:
1. Message Duplication: The same message is stored in both queues, which means you're duplicating data and increasing storage costs.
2. Inconsistent Delivery: If one queue is temporarily unavailable, the message still reaches the other queue. This creates data inconsistency. In our example, it can mean that the analytics DB got updated but the search index doesn't have any record of the video. Now creators are confused because their analytics show the video exists, but no one can find it via search. Debugging this becomes a nightmare.

What we want here is a "publish to one, read by many" paradigm where:
- The producer publishes just once, without worrying about how many services are consuming 
- Multiple consumers can independently read the same message
- If a message is lost, it‚Äôs lost for all consumers, maintaining consistency

This is exactly what event streams offer.

An event stream (a.k.a. message streams) is like a real-time append-only log of small messages (events), such as video uploads. Each new event is written to the end of the stream. Think of it like a constantly growing file where new entries are always added at the bottom. Unlike queues:
- Messages aren't deleted when read.
- Multiple consumers can read the same event, each at their own pace.
- A consumer can even rewind and reread older events if needed.

One of the most widely used event streaming systems is Apache Kafka. Originally built at LinkedIn and later open sourced, Kafka is a distributed, fault-tolerant log system. It is designed for high-throughput and low-latency messaging.

## Kafka 101: Core Concepts

Let‚Äôs take a bird‚Äôs eye view of how Kafka producers and consumers interact (similar to what we did with RabbitMQ):

1. A producer sends an event to a topic. Topics are different categories where events go. More formally, they are named streams of events, e.g., `user-signups`.

2. Kafka splits each topic into partitions, which are:
    - Ordered: Events are written sequentially.
    - Immutable: Once written, events can‚Äôt be changed.
    
    Kafka writes each message/event sent to a topic in one of the topics's partitions. Each event in a partition gets a unique ID called an offset, which lets Kafka and consumers track message positions over time.

3. Consumers read partitions independently at their own pace. Kafka uses offsets to track how far each consumer has read, allowing:
    - Retry or replay of past events.
    - Independent progress for each consumer.

> üìù Note: The Kafka Broker is the server that stores events in partitions and handles requests from producers and consumers You typically have multiple brokers forming a Kafka cluster.

#### Example: Food Delivery App 

Let‚Äôs say you're building a food delivery app.
1. User places an order -> Your backend app creates an event `OrderPlaced`.
2. The app sends the event to Kafka‚Äôs `orders` topic.
3. Kafka stores the event in one of the topic‚Äôs partitions.
4. A consumer service (like a notification service) reads the `OrderPlaced` event.
5. It processes the event (e.g., sends a confirmation text). 
6. Kafka keeps the event stored even after it‚Äôs read, allowing another consumer (like analytics) to read it later.


## Consumer Groups & Partition Assignment

From a consumer's perspective:
- It subscribes to a topic and not a partition
- It is assigned a partition by Kafka

If your notification service has multiple replicas (for scalability), each replica is considered to be a part of the same consumer group, say `notifications`. By design, Kafka will allow only one consumer of a consumer group to consume from a partition of any topic. This means that each replica of our notification service consumes from a different partition of the `orders` topic. This ensures that each message is only consumed once per consumer group. 

Let's look the some scenarios:
1. If the number of consumers in a consumer group is the same as the number of partitions in a topic, each consumer gets assigned one partition.
2. If the number of consumers in a consumer group is less than the number of partitions in a topic, some consumers will get assigned more than one partition. This maintains full coverage of events, but leads to uneven load. 
3. If the number of consumers in a consumer group is more than the number of partitions in a topic, some consumers will get assigned one partition and some consumers will stay idle.

> üö® Key Rule: Within a single consumer group, only one consumer reads from each partition.

This strategy ensures:
- Each replica is assigned a unique partition of the topic.
- No duplication: the same message is not processed twice within the group.
- Parallelism: different replicas can process different partitions concurrently.

## Kafka Brokers

When you run Kafka, it‚Äôs not just about topics and consumers - it‚Äôs also about where and how Kafka stores and serves data. That job is handled by Kafka brokers.

If we just have one broker where all our partitions live, and all the producers and consumers connect to the same broker, our entire Kafka setup depends on that one machine. If it crashes (hardware failure, network issue, software bug), everything stops, that is, producers can‚Äôt write, and consumers can‚Äôt read. This is known as a single point of failure.

To avoid this, Kafka typically runs on multiple brokers, which together form a Kafka cluster. Essentially, a Kafka cluster is a group of brokers working together to handle data, distribute load, and provide fault tolerance.

#### Example: Partitions and Replication

When creating a topic, we have to specify the number of partitions we want in the topic and the replication factor for the topic. The replication factor is the number of replicas we want for each partition. Let's understand this through an example: Suppose our Kafka cluster has 3 brokers (`B1`, `B2`, `B3`). We create a topic named `orders` and configure it to have 3 partitions (`P1`, `P2`, `P3`), and a replication factor of 2 (i.e. each partition should have two copies). Then Kafka might distribute the partitions as follows:
- `P1` lives on brokers `B1` and `B3`
- `P2` lives on brokers `B2` and `B3`
- `P3` lives on brokers `B1` and `B2`

This distribution would ensure that if broker `B1` is down, partitions `P1` and `P3` that live on it are still available on other brokers that clients can connect to. 

### Partition Leaders & Followers

Each partition has:
- One leader replica: Handles all reads and writes
- One or more follower replicas: Stay in sync, but don‚Äôt serve client requests

In our above example, it could look like:
- `P1` lives on `B1` (leader) and `B3` (follower)
- `P2` lives on `B2` (follower) and `B3` (leader)
- `P3` lives on `B1` (follower) and `B2` (leader)

Clients (producers and consumers) only talk to the leader broker of a partition.

Now, if `B1` goes down, Kafka will choose a new leader for `P1`. Since `P1` lives on `B3` as well, all the clients would be served from `B3` now. Thus, if the leader broker fails, a follower takes over. This ensures high availability and no data loss, assuming replicas are in sync.

Kafka keeps replica partitions in sync using the following replication mechanism between the leader and its followers:
1. The leader partition handles all reads and writes, and the followers continuously fetch data from the leader to stay up to date. 
2. These followers pull new messages and write them in the same order to their local log. 
3. Kafka keeps track of which followers are in sync - these are called in-sync replicas (ISRs). Only when a message is written to all ISRs is it considered ‚Äúcommitted.‚Äù This ensures high durability, so even if the leader fails, a fully up-to-date replica can safely take over.


If a broker is the leader for multiple partitions and it goes down, all the producers and consumers connected to those partitions will need to switch to the new leader for each partition. This is how Kafka handles it:
1. Kafka triggers leader election for each affected partition.
2. A new leader is chosen from the in-sync replicas (ISRs).
3. Producers and consumers learn about the new leaders through the Kafka client library, which handles re-routing without manual intervention.

This ensures minimal disruption and no data loss.

### How Kafka Manages the Cluster

To keep the cluster running smoothly, Kafka needs to:
- Track which brokers are alive
- Manage topic and partition metadata
- Handle partition leader elections

This cluster metadata is centrally stored and managed, and this depends on which version of Kafka you‚Äôre using. Previously, this was accomplished using ZooKeeper, but now it is managed by KRaft controller (built-in Kafka process). These systems store critical metadata like:
- Partition leader info
- List of replicas per partition
- Broker liveness
- Topic configs

## Kafka Offsets

An offset is simply a number that marks the position of a message in a Kafka partition. If the Kafka partition is a playlist, the offset is the track number.

You can think of each partition as an append-only log file. The offset represents the position of an event in that file. So, the first event in a partition gets offset `0`, the second gets `1`, and so on. These offsets are immutable‚Äîonce assigned, they never change. Both Kafka and consumers use offsets to keep track of which messages have been read and which are next in line.

> üìù Note: Unlike RabbitMQ, which supports both push and pull-based consumption, Kafka is strictly pull-based. This design allows different consumer groups to read at their own pace from the same partition, enables batching, supports re-reading old data by seeking back to previous offsets, and helps Kafka scale better because it doesn't need to manage delivery state or queues for every consumer.

### How Kafka Stores Partitions, and Messages

Under the hood, each Kafka partition corresponds to a directory on the broker‚Äôs filesystem. Inside this directory, messages are stored as records within multiple segment files. Instead of keeping all messages in one giant file, Kafka breaks the log into smaller chunks called segments to make managing and accessing data more efficient.

Each segment is actually a pair of files:
1. A data file (e.g., `00000000000000000000.log`) that stores the actual messages.
2. An index file (e.g., `00000000000000000000.index)` that maps message offsets to byte positions in the data file for faster lookup.

Kafka creates a new segment file when:
- The current segment reaches a configured size (e.g., 1 GB), or
- A certain amount of time has passed (e.g., 1 hour)

Segment files are named using the base offset of the first message they contain. For example:
```
00000000000000000000.log starts at offset 0  
00000000000000100000.log starts at offset 100000  
00000000000000200000.log starts at offset 200000  
```

This approach helps Kafka:
- Efficiently delete old data by removing entire segment files at once.
- Quickly find messages by offset using the index files. In fact, Kafka‚Äôs index lookups operate in O(1) time within a segment, meaning it stays super fast even with millions of messages.

Kafka also leverages the operating system‚Äôs page cache to keep recently accessed data in memory (RAM), which helps speed up reads. The page cache is a built-in OS feature (on Linux, Windows, etc.) that stores copies of disk files in memory for faster access.

When Kafka writes messages to disk, the OS automatically caches this data in the page cache just in case it‚Äôs needed again soon. If a consumer requests a message still held in the page cache, the OS can deliver it instantly without hitting the disk.

This design means Kafka can serve reads from memory when possible, without having to manage caching itself. And even when Kafka reads from disk, it does so efficiently by writing and reading data in large, sequential batches.

> üìù Note: Kafka supports batch fetching, which is essential for performance and throughput. Instead of requesting one message at a time, a consumer fetches a whole batch of messages in one go. This is the default and recommended behavior for Kafka consumers.


### How Kafka Stores Offsets (Under the Hood)

Kafka provides two ways for consumers to manage offsets:

#### 1. Consumer commits the offset to Kafka itself (default)
In this mode, Kafka stores offsets in a special internal topic called `__consumer_offsets`. This topic is replicated, durable, and compacted (we‚Äôll see what that means shortly). Offsets are stored per consumer group and per topic-partition. 

Offset commits are just special messages inside the `__consumer_offsets` topic. For example: If a consumer from the group `notifications` is reading from partition 3 of the `orders` topic and has processed up to offset `105`, Kafka stores something like:
```
notifications has read up to offset 105 in orders[3]
```

As long as everything is running normally (no consumer crashes or reassignments), the Kafka consumer client library keeps track of the offsets in memory and knows which partitions the consumer is assigned to. When it fetches messages, it simply asks Kafka: *"Give me messages starting from offset `106`"*. Kafka will respond with messages: `106`, `107`, `108`, and so on.

If the application restarts (due to a reboot or crash), Kafka retrieves the last committed offset from the `__consumer_offsets` topic and tells the consumer: *"Hey, your last known position was `105`. Start from `106`."* <br>
This recovery is done via a lightweight request to the Kafka brokers.
<br><br>

| Function                                                        | Who Handles It                | Where It Lives                              |
| --------------------------------------------------------------- | ----------------------------- | ------------------------------------------- |
| **Track current offset** (as you fetch new messages)            | Kafka consumer client         | In memory (no code from you needed)         |
| **Store committed offset** (so you can recover after a restart) | Kafka + client (configurable) | Kafka's internal topic `__consumer_offsets` |

<br>
#### 2. Consumer stores offsets externally
While Kafka‚Äôs internal offset management is commonly used in modern systems, consumers can also manage offsets themselves. In this setup, the application is responsible for storing offsets (per consumer group per topic-partition) in an external data store like Redis, MySQL, etc.

### How Offsets Are Committed

Consumers also have two choices for when to commit offsets:

#### 1. Automatic commit
By default, Kafka automatically commits offsets at regular intervals (every 5 seconds). This means:
- If a crash happens during message processing, and Kafka commits offsets before the crash, the message might be lost (i.e., not reprocessed).
- If a crash happens after message processing but before the commit, the same message might get processed again.

This is not the best choice for critical or idempotent systems (which require exactly-once processing).

#### 2. Manual commit
Here, the consumer explicitly commits the offset only after successful message processing. This provides more control and is better for preventing data loss or duplication, especially in important or idempotent systems.

### Offset and Rebalancing

Kafka will reassign partitions (a process called rebalancing) in the following situations:
- Consumer application crashes or is restarted
- A new consumer joins the group
- A consumer leaves the group
- A heartbeat timeout occurs (the consumer is slow or paused)

When this happens, Kafka redistributes the partitions among consumers. Each consumer might start handling new partitions it wasn‚Äôt responsible for earlier. Thanks to offsets, each consumer can resume from the last committed offset for its new partition(s), ensuring smooth failover and fault tolerance. Offsets are key to Kafka's resilience and reliability.

## Compacted Logs

By default, Kafka keeps all messages in a topic for a fixed period (e.g., 7 days). However, some topics, like `__consumer_offsets`, use a different retention strategy called log compaction.

With log compaction, Kafka doesn‚Äôt delete messages based on time. Instead, it keeps only the most recent value for each key, removing older versions. In the the `__consumer_offsets` topic:
```
Key = consumer_group + topic + partition
Value = last committed offset
```

This way, only the latest committed offset for each consumer group per partition is stored, and the topic doesn‚Äôt grow indefinitely.

You can also configure your own topics to use compaction by setting: `cleanup.policy=compact`. This is perfect for scenarios like:
- Inventory counts (latest stock per product)
- User profiles (latest info per user)
- Account balance (latest balance per account)

Under the hood, Kafka  periodically runs a compaction job in the background to compact topics. This job keeps only the most recent value for each key and deletes the older ones. If the latest value is null, it deletes the key entirely.

It's important to note that compaction is not the same as compression. Compaction removes old records for the same key, keeping only the latest and compression shrinks size of messages using algorithms like gzip or snappy. They can be used together, but they solve different problems.

## How Producers and Consumer Groups Connect to Kafka?

Before messages can flow through Kafka, producers and consumers need to connect to the Kafka cluster. Producers send messages to specific topics, while consumers (organized into consumer groups) subscribe to those topics to read the messages. Let‚Äôs look at how these connections work behind the scenes.

### How Does a Kafka Producer Connect?

When a producer connects to a Kafka cluster:
1. Knocking on the door: The producer starts by contacting a bootstrap server. This is just the address of one or more Kafka brokers in the cluster that help the producer find the rest of the cluster.
2. Getting the cluster info: The broker that the producer reached out to gives the producer a list of all the other brokers in the cluster and information about topics and partitions. The producer now knows which broker is the leader for each partition. The Kafka client library handles all this behind the scenes.
3. Sending the Message: Now the producer is ready to send messages:
    + Specific Partition: If the producer specifies a particular partition, the client library will send the message to the leader of the partition specified
    + Using a Key: Instead, if a key is specified by the producer (like `client_id`), the client library will hash the key to choose a partition and send the message the leader of that partition. This means that all messages with the same key will go to the same partition. 
    + No Key, No Partition: If no key or partition is specified, the client library will send the message to a partition chosen using round-robin.

### How Does a Kafka Consumer Group Connect?
When a consumer group connects to a Kafka cluster, Kafka assigns it a Group Coordinator, which is a single broker responsible for managing the group‚Äôs membership (which consumers are in the group) and partition assignments (which partitions should each should consumer read from).

##### Step 1: Group Coordinator Assignment
Kafka uses a deterministic process based on the group ID (specified by the consumers when connecting to the cluster). It applies a hash function to the group ID and maps it to one of the brokers in the cluster. That broker becomes the Group Coordinator for the group.

Even if your Kafka cluster has 10 brokers, each consumer group will be managed by just one of those brokers - the "Group Coordinator". What does "manage" here mean?
1. Keeps a list of all active consumers in the group.
2. Consumers periodically send heartbeats to signal they‚Äôre still alive. If one fails to send, it‚Äôs considered dead and removed.
3. If a consumer joins, leaves, crashes, or if the number of topic partitions changes, the coordinator triggers a rebalance.
4. It works with the Group Leader to assign partitions to consumers.

##### Step 2: Group Leader Election
In addition to the coordinator, Kafka elects one consumer from the group to act as the Group Leader. This is done during a join phase, usually by picking the first consumer that joins. The Group Leader‚Äôs job is to:
1. Collect metadata from each consumer (e.g., what topics and partitions they are already subscribed to, if any).
2. Run the partition assignment strategy (like round-robin or range assignment).
3. Propose a mapping of partitions to consumers.

The Group Coordinator receives this proposal, finalizes it, and sends individual assignments back to each consumer.

> üìù Note: The Group Leader role is temporary. It‚Äôs elected every rebalance and isn‚Äôt stored or remembered by Kafka.

##### Example
Let's understand this from our previous example where our Kafka cluster has 3 brokers (`B1`, `B2`, `B3`). Our topic `orders` has 3 partitions (`P1`, `P2`, `P3`), and a replication factor of 2. This is how the partitions are distributed:
- `P1` lives on `B1` and `B3`
- `P2` lives on `B2` and `B3`
- `P3` lives on `B1` and `B2`

A consumer group with 3 consumers `C1`, `C2`, and `C3` joins. On joining,
- Kafka assigns `B2` as the Group Coordinator
- Kafka elects `C3` as the Group Leader 

The leader now gathers metadata from the consumers and proposes the following assignment to the Group Coordinator: 
- `C1` will read `P3` 
- `C2` will read `P1`
- `C3` will read `P2` 

The group Coordinator will approve this assignment and send each consumer their specific assignment.

##### Rebalancing
Any time a consumer joins or leaves, or the topic‚Äôs partition count changes, Kafka triggers a rebalance. During rebalancing:
1. The Group Leader is reelected
2. Consumers pause fetching
3. New assignments are calculated and sent out

> üìù Note: Kafka stores the Group Coordinator mapping for each consumer group in the `__consumer_offsets` topic. The group leader is an ephemeral position, dynamically created just for the purpose of assignment, and is not stored anywhere. 

### How Does a Kafka Consumer Connect?
When a Kafka consumer starts up:
1. It first connects to a bootstrap broker, just like a producer does, to discover the full cluster layout.
2. Using its group ID, the consumer learns which broker is the Group Coordinator for its consumer group.
3. It then joins the consumer group by talking to that Group Coordinator.
4. The Group Coordinator picks one of the consumers as the Group Leader, who gathers metadata and proposes a partition assignment.
5. The Group Coordinator finalizes the assignments and tells each consumer which partition(s) it should consume from.
6. The consumer then connects to the leader broker of its assigned partition(s) and starts consuming messages.

All of this coordination and communication is handled behind the scenes by the Kafka client library, so you typically don‚Äôt need to implement it yourself.








