---
layout: post
title: "Kafka"
subtitle: ""
tags: ["system-design"]
readtime: true
---

In the previous post, we looked at what async messaging is, why is it needed, and how we can leverage message brokers for async communication. Imagine that you are developing a video ingestion application. When a creator submits a video, you want to do two things:
- Index it in your search index DB
- Update the analytics DB with stats on how many videos has the creator published, how many minutes has the creator published, etc

If you are using message brokers, you'll have to use two queues bound to a topic exchange as if you connect two separate consumers to a single queue, each messages is delivered to only one of them. The issue with using two queues is that:
1. Each message is getting duplicated in both queues
2. If one queue is temporarily unavailable, the message only makes it to one of the queues. In our example, it can mean that the analytics DB got updated but the search index doesn't have any record of the video. This is a nightmare for the creators and the developers as well.

What we want here is a "publish to one, consumed by many" paradigm where the producer only publishes to one queue but multiple consumers can read from the queue. It would ensure that if a message is dropped, it is dropped for all consumers. Events Streams (also called Message Streams) enable us to achieve this. 

An event stream or a message stream is simply a real-time sequence of events (small discrete messages), like a log that keeps appending new entries as they come. One of the most commonly used Events Streams is Apache Kafka. It is a distributed event streaming platform developed by LinkedIn originalyy and later open sourced. You can think of Kafka as a distributed commit log where each new event is written at the end, and multiple clients can read the log at their own pace.

## Kafka 101: Core Concepts

As we did with RabbitMQ, the best way to understand terminology is to take a bird's eye view of how producers and consumers write and read from Kafka:
1. A producer publishes an event to a topic. Topics are different categories where events go. More formally, they are named streams of events, e.g., `user-signups`.
2. Kafka splits each topic into smaller pieces called partitions, which are ordered and immutable. Kafka writes each message/event sent to a topic in one of the topics's partitions. Every event in a partition gets a number called an offset, which is just a way of Kafka to track the messages in the partition.
3. A partition can be read by different consumers, each consuming the partition at its own pace. Kafka uses the offsets to track which message each consumer consumed last.

Let’s walk through a simple example: You're building a food delivery app like DoorDash.
1. User places an order -> Your backend app creates an event like `OrderPlaced`.
2. The app is a producer -> it sends the event to Kafka’s `orders` topic.
3. Kafka stores the event in one of the topic’s partitions.
4. Your consumer app (maybe a notification service) reads the `OrderPlaced` event.
5. It processes the event (e.g., sends a confirmation text). Kafka keeps the event stored even after it’s read, so another consumer (like an analytics service) can also read it later.

The Kafka server that stores events and handles requests is called the Broker. 

From the consumer perspective, a consumer service subscribes to a topic and not a partition - it is assigned a partition by the Kafka broker. In a distributed system, you typically have multiple replicas of each service, let's say the notificitation service. When each replica of the notification service connects to Kafka, it tells Kafka *"Hey! I want to connect to `orders` topic and I belong to `notifications` consumer group"*. By design, Kafka will allow only one consumer of a consumer group to consume from a partition of any topic. This means that each replica of our notification service consumes from a different partition of the `orders` topic. This ensures that each message is only consumed once per consumer group. 

Let's look the some scenarios:
1. If the number of consumers in a consumer group is the same as the number of partitions in a topic, each consumer gets assigned one partition.
2. If the number of consumers in a consumer group is less than the number of partitions in a topic, some consumers will get assigned more than one partition. This maintains full coverage of events, but leads to uneven load. 
3. If the number of consumers in a consumer group is more than the number of partitions in a topic, some consumers will get assigned one partition and some consumers will stay idle.

Kafka ensures no duplication within a consumer group (the same message is not read by different consumers within a consumer group) and parallelism across partitions (different replicas are subscribed to different partitions).

### Kafka Brokers

If we just have one broker where all our partitions live, and all the producers and consumers connect to the same broker, we expose our system to a single point of failure. Any harware or software fault would mean that all the clients of our broker come to a halt. To avoid this, Kafka typicalls runs on multiple brokers, forming a cluster. Essentially, a Kafka cluster is a group of Kafka brokers working together. 

When creating a topic, we have to specify the number of partitions we want in the topic and the replication factor for the topic. The replication factor specified the number of replicas of each partition we want. Let's understand this through an example: Suppose our Kafka cluster has 3 brokers (`B1`, `B2`, `B3`). We create a topic named `orders` and configure it to have 3 partitions (`P1`, `P2`, `P3`), and a replication factor of 2. Then Kafka might distribute the partitions as follows:
- `P1` lives on `B1` and `B3`
- `P2` lives on `B2` and `B3`
- `P3` lives on `B1` and `B2`

This distribution would ensure that if broker `B1` is down, partitions `P1` and `P3` that live on it are still available on other brokers that clients can connect to. Out of all the replicas of a partition, Kafka allows only one replica to directly connected to producers and listeners. This means that all the reads and writes happen to only one replica and then the other replicas are synced to keep them up to date. The broker where this partition lives is called the leader broker of the partition (a.k.a. partition leader). In our above example, it could look like:
- `P1` lives on `B1` and `B3`; `B1` is the leader broker
- `P2` lives on `B2` and `B3`; `B3` is the leader broker
- `P3` lives on `B1` and `B2`; `B2` is leader broker

Now, if `B1` goes down, Kafka will choose a new leader for `P1`. Since `P1` lives on `B3` as well, all the clients would be served from `B3` now. Thus, if the leader broker fails, a follower takes over.

All the brokers in a cluster need to be managed, that is, Kafka needs to:
- Know which brokers are alive
- Store and manage topic and partitions metadata
- Facilitate leader election for partitions

Previously, this was accomplished using ZooKeeper, but now it is managed by KRaft controller. I'll discus them in some other post to keep this one more focused on Kafka.

Information on partition leader and replica partitions for each topic-partition is also stored in centrally in ZooKeeper (older Kafka version) or the KRaft controller (newer Kafka versions).

## Kafka Offsets

An offset is just a number that marks the position of a message in a Kafka partition. Each partition can be thought of as an append-only-log file. An offset is the position of an event in this log file. So the first event in a partition gets assigned an offset of 0, the second message gets assigned an offset of 1, and so on. This offset is immutable, that is, messages never change place. Offsets are used by the consumer and the kafka to decide which message the consumer is supposed to read next.

> 📝 Note: Unlike RabbitMQ, which supports both push and pull based consumption, Kafka is strictly pull-based. This design enables different consumer groups to consumer at their own pace from the same partition, enables Kafka to support batching, allows consumers to re-read old data by seeking back to older offsets, and improves scalability of Kafka as it doesn't have to manage queues and delivery state per consumer.

### How Kafka Stores Offsets (Under the Hood)

Kafka gives consumers two options for managing offsets:

#### 1. Consumer commits the offset to Kafka itself (default)
In this mode, Kafka stores the offsets in a special internal topic called `__consumer_offsets`. This topic is replicated, durable, and compacted (we'll see what it means) and offsets are stored per consumer group, per topic-partition. Offset commits are just special messages in the `__consumer_offsets` topic. For example: For a consumer from the consumer group `notifications`, consuming from the 3rd partition in the topic `orders`, if the last read message has an offset of 105, Kafka stores: "notifications has read up to offset 105 in orders[3]"

Now, if everything is running smoothly (no consumer restarts, no partition reassignments, etc), the consumer itself tracks the offsets in memory and which partition it is assigned to (this is implemented by the kafka client libraries). Each time it wants to read from Kafka, it asks *"Give me messages starting from offset `106`"*. Kafka will respond with messages: `106`, `107`, `108`, ...

> 📝 Note: Kafka supports batch fetching, and it’s a key feature for performance and efficiency. Instead of asking for one message at a time, a Kafka consumer can request a batch of messages in a single fetch request. Batch fetching is the default and recommended behavior in Kafka consumers for efficiency and throughput.

In the case of an application restart (Reboot / Crash Recovery), Kafka uses the offsets it stored in `__consumer_offsets` to tell the consumer *"Hey, your last known position was `105`. Start from `106`."* Consumers fetch committed offsets using a lightweight request to Kafka brokers.

| Function                                                        | Who Handles It                | Where It Lives                              |
| --------------------------------------------------------------- | ----------------------------- | ------------------------------------------- |
| **Track current offset** (as you fetch new messages)            | Kafka consumer client         | In memory (no code from you needed)         |
| **Store committed offset** (so you can recover after a restart) | Kafka + client (configurable) | Kafka's internal topic `__consumer_offsets` |

#### 2. Consumer stores offsets externally
In most modern systems, Kafka’s internal offset management is the go-to choice. But the consumer can choose to handle everything on their side. To accomplish this, consumers will have to store offsets for each consumer group per topic-partition in some external DB like Redis, MySQL, etc.


### How Offsets Are Committed

Consumers again have two choices when it to comes to when to commit offsets:

#### 1. Automatic commit
Kafka Broker will periodically commit the offsets per consumer group per topic-partition periodically (default every 5 seconds). This means:
- If the consumer crashes while processing a message and the Kafka periodic commit happens during this time, the consumer won't get the message again
- If the consumer crashes after processing a message but before Kafka periodic commit happens, the consumer will have to reconsume the message it has already processed

This is not the best choice for critical or idempotent systems.

#### 2. Manual commit
In this mode, the consumer commits offsets only after successful processing. It gives more control, especially for avoiding message loss or duplication.

### Offset and Rebalancing

Kafka reassigns partitions in the following scenarios:
- Consumer application crashes or is restarted
- A new consumer joins the group
- A consumer leaves the group
- A heartbeat timeout occurs (the consumer is slow or paused)

In any of the above scenarios, Kafka will have to rebalance the group. This means that the partitions of the topics being consumed by the consumer group get reassigned, and consumers may take over partitions they weren’t handling before. Kafka offsets allow the consumers to resume from the last committed offset of each partition for that consumer group. This makes offsets essential for fault tolerance and smooth failover, making the system both, resilient and reliable.

### Compacted Logs

Normally, Kafka keeps every message in a topic for a configured amount of time (like 7 days). But Kafka uses another strategy called log compaction (instead of time-based retention) for some topics, like the special `__consumer_offsets` topic. In this strategy, instead of deleting messages after a time, it deletes old values for the same key, keeping just the most recent update.

This is how Kafka efficiently stores committed offsets in the the `__consumer_offsets` topic, that is, only the last committed offset is stored per consumer group per partition:
```
Key = consumer_group + topic + partition
Value = last committed offset
```

This keeps the topic from growing forever. We can configure any topic we create to be compacted this way by setting `cleanup.policy=compact`. This is great for things like:
- Inventory counts (latest stock per product)
- User profiles (latest info per user)
- Account balance (latest balance per account)

Under the hood, Kafka  periodically runs a compaction job in the background to compact topics. This job keeps only the most recent value for each key and deletes the older ones. If the latest value is null, it deletes the key entirely.

It's important to note that compaction is not the same as compression. Compaction removes old records for the same key, keeping only the latest and compression shrinks size of messages. They can be used together, but they solve different problems.

## How Kafka manages Producers and Consumer Groups?

Let’s walk through both:

### How does a Producer connect?

When a producer connects, it connects to any broker on the cluster (called the bootstrap broker). The broker replies with a list of all brokers in the cluster and metadata about topics and partitions. The producer now knows which broker is the leader for each partition. All of this will be handled by the Kafka client library, so you don't have to code it. If the producer message specifies a particular partition, the client library will send the message to the leader of the partition specified. Instead, if a key is specified, the client library will hash the key to choose a partition and send the message the leader of that partition. This means that all messages with the same key will go to the same partition. If no key or partition is specified, the client library will send the message to a partition chosen using round-robin.

### How does a Consumer Group connect?

When a consumer group joins Kafka, it is assigned a Group Coordinator. Kafka uses an internal mechanism to choose one broker in the cluster to serve as the Group Coordinator for that specific group. The choice is based on the group ID, using a hash function to map it deterministically to a broker. Even if your Kafka cluster has 10 brokers, each consumer group will be managed by just one of those brokers, selected by Kafka. What does manage here mean?
1. Group Coordinator keeps track of all consumers in the group.
2. Manages heartbeats (to know if each consumer in a consumer group is still active).
3. Triggers rebalancing when a new consumer joins, a consumer crashes, or the number of partitions change
4. Coordinates with the Group Leader to get partition assignments.

In addition to a Group Coordinator, Kafka will also pick any one of the consumers in the consumer group to be the Group Leader. This Group Leader consumer collects metadata (e.g., which topic each consumer want to consume) from all consumers in the group, runs a partition assignment algorithm, and proposes which which consumer gets which partitions of the topic. The Group Coordinator then finalizes and distributes this assignment.

Let's understand this from our previous example where our Kafka cluster has 3 brokers (`B1`, `B2`, `B3`). Our topic `orders` has 3 partitions (`P1`, `P2`, `P3`), and a replication factor of 2. This is how the partitions are distributed:
- `P1` lives on `B1` and `B3`
- `P2` lives on `B2` and `B3`
- `P3` lives on `B1` and `B2`

A consumer group with 3 consumers `C1`, `C2`, and `C3` joins Kafka. On joining, it gets assigned `B2` as the Group Coordinator and `C3` is chosen as the Group Leader. The leader now proposes the following assignment to the Group Coordinator: `C1`, `C2`, `C3` will read from partitions `P3`, `P1`, `P2` respectively. The group Coordinator will approve this assignment and send each consumer their specific assignment.

The same process happens even when a rebalance is needed. The Group Coordinator remains the same across rebalances, unless a failure or exceptional condition causes Kafka to reassign it, but the Group Leader is elected every time the group enters a rebalance phase.

Kafka stores the Group Coordinator mapping for each consumer group in the `__consumer_offsets` topic. The group leader is an ephemeral position, dynamically created just for the purpose of assignment, and is not stored anywhere. 

### How does a consumer connect?

A consumer also connects to a boostrap broker. It finds out which broker is the Group Coordinator for its consumer group. It joins the consumer group via that Group Coordinator broker. A Group Leader is chosen by the Group Coordinator, which reassigns the partitions and lets the consumer know which partition it will consume from. The client library on the consumer side will start consuming from the partition leader of that partition.








