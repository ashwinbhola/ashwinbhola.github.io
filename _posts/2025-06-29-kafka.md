---
layout: post
title: "Kafka"
subtitle: ""
tags: ["system-design"]
readtime: true
---

In the previous post, we looked at what asynchronous messaging is, why itâ€™s useful, and how message brokers help decouple services so they can communicate without waiting on each other.

Now, letâ€™s consider a real-world example. Imagine youâ€™re building a video ingestion service. When a creator uploads a new video, your system needs to:
1. Index it in a search database, so users can discover it.
2. Update an analytics database, tracking how many videos and total minutes the creator has published.

At first glance, using a message broker like RabbitMQ seems like a good fit. You might design it like this: 

1. A topic exchange receives video upload events.
2. Two queues are bound to that exchange: one for the search service, and one for analytics.
3. Each queue has its own consumer service listening for messages.

But this approach has problems:
1. Message Duplication: The same message is stored in both queues, which means you're duplicating data and increasing storage costs.
2. Inconsistent Delivery: If one queue is temporarily unavailable, the message still reaches the other queue. This creates data inconsistency. In our example, it can mean that the analytics DB got updated but the search index doesn't have any record of the video. Now creators are confused because their analytics show the video exists, but no one can find it via search. Debugging this becomes a nightmare.

What we want here is a "publish to one, read by many" paradigm where:
- The producer publishes just once, without worrying about how many services are consuming 
- Multiple consumers can independently read the same message
- If a message is lost, itâ€™s lost for all consumers, maintaining consistency

This is exactly what event streams offer.

An event stream (a.k.a. message streams) is like a real-time append-only log of small messages (events), such as video uploads. Each new event is written to the end of the stream. Think of it like a constantly growing file where new entries are always added at the bottom. Unlike queues:
- Messages aren't deleted when read.
- Multiple consumers can read the same event, each at their own pace.
- A consumer can even rewind and reread older events if needed.

One of the most widely used event streaming systems is Apache Kafka. Originally built at LinkedIn and later open sourced, Kafka is a distributed, fault-tolerant log system. It is designed for high-throughput and low-latency messaging.

## Kafka 101: Core Concepts

Letâ€™s take a birdâ€™s eye view of how Kafka producers and consumers interact (similar to what we did with RabbitMQ):

1. A producer sends an event to a topic. Topics are different categories where events go. More formally, they are named streams of events, e.g., `user-signups`.

2. Kafka splits each topic into partitions, which are:
    - Ordered: Events are written sequentially.
    - Immutable: Once written, events canâ€™t be changed.
    
    Kafka writes each message/event sent to a topic in one of the topics's partitions. Each event in a partition gets a unique ID called an offset, which lets Kafka and consumers track message positions over time.

3. Consumers read partitions independently at their own pace. Kafka uses offsets to track how far each consumer has read, allowing:
    - Retry or replay of past events.
    - Independent progress for each consumer.

> Note: The Kafka Broker is the server that stores events in partitions and handles requests from producers and consumers You typically have multiple brokers forming a Kafka cluster.

#### Example: Food Delivery App 

Letâ€™s say you're building a food delivery app.
1. User places an order -> Your backend app creates an event `OrderPlaced`.
2. The app sends the event to Kafkaâ€™s `orders` topic.
3. Kafka stores the event in one of the topicâ€™s partitions.
4. A consumer service (like a notification service) reads the `OrderPlaced` event.
5. It processes the event (e.g., sends a confirmation text). 
6. Kafka keeps the event stored even after itâ€™s read, allowing another consumer (like analytics) to read it later.


### Consumer Groups & Partition Assignment

From a consumer's perspective:
- It subscribes to a topic and not a partition
- it is assigned a partition by Kafka

If your notification service has multiple replicas (for scalability), each replica is considered to be a part of the same consumer group, say `notifications`. By design, Kafka will allow only one consumer of a consumer group to consume from a partition of any topic. This means that each replica of our notification service consumes from a different partition of the `orders` topic. This ensures that each message is only consumed once per consumer group. 

Let's look the some scenarios:
1. If the number of consumers in a consumer group is the same as the number of partitions in a topic, each consumer gets assigned one partition.
2. If the number of consumers in a consumer group is less than the number of partitions in a topic, some consumers will get assigned more than one partition. This maintains full coverage of events, but leads to uneven load. 
3. If the number of consumers in a consumer group is more than the number of partitions in a topic, some consumers will get assigned one partition and some consumers will stay idle.

> ðŸš¨ Key Rule: Within a single consumer group, only one consumer reads from each partition.

This strategy ensures:
- Each replica is assigned a unique partition of the topic.
- No duplication: the same message is not processed twice within the group.
- Parallelism: different replicas can process different partitions concurrently.

### Kafka Brokers

When you run Kafka, itâ€™s not just about topics and consumers - itâ€™s also about where and how Kafka stores and serves data. That job is handled by Kafka brokers.

If we just have one broker where all our partitions live, and all the producers and consumers connect to the same broker, our entire Kafka setup depends on that one machine. If it crashes (hardware failure, network issue, software bug), everything stops, that is, producers canâ€™t write, and consumers canâ€™t read. This is known as a single point of failure.

To avoid this, Kafka typically runs on multiple brokers, which together form a Kafka cluster. Essentially, a Kafka cluster is a group of brokers working together to handle data, distribute load, and provide fault tolerance.

#### Example: Partitions and Replication

When creating a topic, we have to specify the number of partitions we want in the topic and the replication factor for the topic. The replication factor is the number of replicas we want for each partition. Let's understand this through an example: Suppose our Kafka cluster has 3 brokers (`B1`, `B2`, `B3`). We create a topic named `orders` and configure it to have 3 partitions (`P1`, `P2`, `P3`), and a replication factor of 2 (i.e. each partition should have two copies). Then Kafka might distribute the partitions as follows:
- `P1` lives on brokers `B1` and `B3`
- `P2` lives on brokers `B2` and `B3`
- `P3` lives on brokers `B1` and `B2`

This distribution would ensure that if broker `B1` is down, partitions `P1` and `P3` that live on it are still available on other brokers that clients can connect to. 

#### Partition Leaders & Followers

Each partition has:
- One leader replica: Handles all reads and writes
- One or more follower replicas: Stay in sync, but donâ€™t serve client requests

In our above example, it could look like:
- `P1` lives on `B1` (leader) and `B3` (follower)
- `P2` lives on `B2` (follower) and `B3` (leader)
- `P3` lives on `B1` (follower) and `B2` (leader)

Clients (producers and consumers) only talk to the leader broker of a partition.

Now, if `B1` goes down, Kafka will choose a new leader for `P1`. Since `P1` lives on `B3` as well, all the clients would be served from `B3` now. Thus, if the leader broker fails, a follower takes over. This ensures high availability and no data loss, assuming replicas are in sync.

Kafka keeps replica partitions in sync using the following replication mechanism between the leader and its followers:
1. The leader partition handles all reads and writes, and the followers continuously fetch data from the leader to stay up to date. 
2. These followers pull new messages and write them in the same order to their local log. 
3. Kafka keeps track of which followers are in sync - these are called in-sync replicas (ISRs). Only when a message is written to all ISRs is it considered â€œcommitted.â€ This ensures high durability, so even if the leader fails, a fully up-to-date replica can safely take over.


If a broker is the leader for multiple partitions and it goes down, all the producers and consumers connected to those partitions will need to switch to the new leader for each partition. This is how Kafka handles it:
1. Kafka triggers leader election for each affected partition.
2. A new leader is chosen from the in-sync replicas (ISRs).
3. Producers and consumers learn about the new leaders through the Kafka client library, which handles re-routing without manual intervention.

This ensures minimal disruption and no data loss.

#### How Kafka Manages the Cluster

To keep the cluster running smoothly, Kafka needs to:
- Track which brokers are alive
- Manage topic and partition metadata
- Handle partition leader elections

This cluster metadata is centrally stored and managed, and this depends on which version of Kafka youâ€™re using. Previously, this was accomplished using ZooKeeper, but now it is managed by KRaft controller (built-in Kafka process). These systems store critical metadata like:
- Partition leader info
- List of replicas per partition
- Broker liveness
- Topic configs

## Kafka Offsets

An offset is just a number that marks the position of a message in a Kafka partition. Each partition can be thought of as an append-only-log file. An offset is the position of an event in this log file. So the first event in a partition gets assigned an offset of 0, the second message gets assigned an offset of 1, and so on. This offset is immutable, that is, messages never change place. Offsets are used by the consumer and the kafka to decide which message the consumer is supposed to read next.

> ðŸ“ Note: Unlike RabbitMQ, which supports both push and pull based consumption, Kafka is strictly pull-based. This design enables different consumer groups to consumer at their own pace from the same partition, enables Kafka to support batching, allows consumers to re-read old data by seeking back to older offsets, and improves scalability of Kafka as it doesn't have to manage queues and delivery state per consumer.

### How Kafka Stores Offsets (Under the Hood)

Kafka gives consumers two options for managing offsets:

#### 1. Consumer commits the offset to Kafka itself (default)
In this mode, Kafka stores the offsets in a special internal topic called `__consumer_offsets`. This topic is replicated, durable, and compacted (we'll see what it means) and offsets are stored per consumer group, per topic-partition. Offset commits are just special messages in the `__consumer_offsets` topic. For example: For a consumer from the consumer group `notifications`, consuming from the 3rd partition in the topic `orders`, if the last read message has an offset of 105, Kafka stores: "notifications has read up to offset 105 in orders[3]"

Now, if everything is running smoothly (no consumer restarts, no partition reassignments, etc), the consumer itself tracks the offsets in memory and which partition it is assigned to (this is implemented by the kafka client libraries). Each time it wants to read from Kafka, it asks *"Give me messages starting from offset `106`"*. Kafka will respond with messages: `106`, `107`, `108`, ...

> ðŸ“ Note: Kafka supports batch fetching, and itâ€™s a key feature for performance and efficiency. Instead of asking for one message at a time, a Kafka consumer can request a batch of messages in a single fetch request. Batch fetching is the default and recommended behavior in Kafka consumers for efficiency and throughput.

In the case of an application restart (Reboot / Crash Recovery), Kafka uses the offsets it stored in `__consumer_offsets` to tell the consumer *"Hey, your last known position was `105`. Start from `106`."* Consumers fetch committed offsets using a lightweight request to Kafka brokers.

| Function                                                        | Who Handles It                | Where It Lives                              |
| --------------------------------------------------------------- | ----------------------------- | ------------------------------------------- |
| **Track current offset** (as you fetch new messages)            | Kafka consumer client         | In memory (no code from you needed)         |
| **Store committed offset** (so you can recover after a restart) | Kafka + client (configurable) | Kafka's internal topic `__consumer_offsets` |

#### 2. Consumer stores offsets externally
In most modern systems, Kafkaâ€™s internal offset management is the go-to choice. But the consumer can choose to handle everything on their side. To accomplish this, consumers will have to store offsets for each consumer group per topic-partition in some external DB like Redis, MySQL, etc.


### How Offsets Are Committed

Consumers again have two choices when it to comes to when to commit offsets:

#### 1. Automatic commit
Kafka Broker will periodically commit the offsets per consumer group per topic-partition periodically (default every 5 seconds). This means:
- If the consumer crashes while processing a message and the Kafka periodic commit happens during this time, the consumer won't get the message again
- If the consumer crashes after processing a message but before Kafka periodic commit happens, the consumer will have to reconsume the message it has already processed

This is not the best choice for critical or idempotent systems.

#### 2. Manual commit
In this mode, the consumer commits offsets only after successful processing. It gives more control, especially for avoiding message loss or duplication.

### Offset and Rebalancing

Kafka reassigns partitions in the following scenarios:
- Consumer application crashes or is restarted
- A new consumer joins the group
- A consumer leaves the group
- A heartbeat timeout occurs (the consumer is slow or paused)

In any of the above scenarios, Kafka will have to rebalance the group. This means that the partitions of the topics being consumed by the consumer group get reassigned, and consumers may take over partitions they werenâ€™t handling before. Kafka offsets allow the consumers to resume from the last committed offset of each partition for that consumer group. This makes offsets essential for fault tolerance and smooth failover, making the system both, resilient and reliable.

### Compacted Logs

Normally, Kafka keeps every message in a topic for a configured amount of time (like 7 days). But Kafka uses another strategy called log compaction (instead of time-based retention) for some topics, like the special `__consumer_offsets` topic. In this strategy, instead of deleting messages after a time, it deletes old values for the same key, keeping just the most recent update.

This is how Kafka efficiently stores committed offsets in the the `__consumer_offsets` topic, that is, only the last committed offset is stored per consumer group per partition:
```
Key = consumer_group + topic + partition
Value = last committed offset
```

This keeps the topic from growing forever. We can configure any topic we create to be compacted this way by setting `cleanup.policy=compact`. This is great for things like:
- Inventory counts (latest stock per product)
- User profiles (latest info per user)
- Account balance (latest balance per account)

Under the hood, Kafka  periodically runs a compaction job in the background to compact topics. This job keeps only the most recent value for each key and deletes the older ones. If the latest value is null, it deletes the key entirely.

It's important to note that compaction is not the same as compression. Compaction removes old records for the same key, keeping only the latest and compression shrinks size of messages. They can be used together, but they solve different problems.

## How Producers and Consumer Groups Connect to Kafka?

Letâ€™s walk through both:

### How Does a Kafka Producer Connect?

When a producer connects to a Kafka cluster:
1. Knocking on the door: The producer starts by contacting a bootstrap server. This is just the address of one or more Kafka brokers in the cluster that help the producer find the rest of the cluster.
2. Getting the cluster info: The broker that the producer reached out to gives the producer a list of all the other brokers in the cluster and information about topics and partitions. The producer now knows which broker is the leader for each partition. The Kafka client library handles all this behind the scenes.
3. Sending the Message: Now the producer is ready to send messages:
    + Specific Partition: If the producer specifies a particular partition, the client library will send the message to the leader of the partition specified
    + Using a Key: Instead, if a key is specified by the producer (like `client_id`), the client library will hash the key to choose a partition and send the message the leader of that partition. This means that all messages with the same key will go to the same partition. 
    + No Key, No Partition: If no key or partition is specified, the client library will send the message to a partition chosen using round-robin.

### How Does a Kafka Consumer Group Connect?
When a consumer group connects to a Kafka cluster, Kafka assigns it a Group Coordinator, which is a single broker responsible for managing the groupâ€™s membership (which consumers are in the group) and partition assignments (which partitions should each should consumer read from).

##### Step 1: Group Coordinator Assignment
Kafka uses a deterministic process based on the group ID (specified by the consumers when connecting to the cluster). It applies a hash function to the group ID and maps it to one of the brokers in the cluster. That broker becomes the Group Coordinator for the group.

Even if your Kafka cluster has 10 brokers, each consumer group will be managed by just one of those brokers - the "Group Coordinator". What does "manage" here mean?
1. Keeps a list of all active consumers in the group.
2. Consumers periodically send heartbeats to signal theyâ€™re still alive. If one fails to send, itâ€™s considered dead and removed.
3. If a consumer joins, leaves, crashes, or if the number of topic partitions changes, the coordinator triggers a rebalance.
4. It works with the Group Leader to assign partitions to consumers.

##### Step 2: Group Leader Election
In addition to the coordinator, Kafka elects one consumer from the group to act as the Group Leader. This is done during a join phase, usually by picking the first consumer that joins. The Group Leaderâ€™s job is to:
1. Collect metadata from each consumer (e.g., what topics and partitions they are already subscribed to, if any).
2. Run the partition assignment strategy (like round-robin or range assignment).
3. Propose a mapping of partitions to consumers.

The Group Coordinator receives this proposal, finalizes it, and sends individual assignments back to each consumer.

> Note: The Group Leader role is temporary. Itâ€™s elected every rebalance and isnâ€™t stored or remembered by Kafka.

##### Example
Let's understand this from our previous example where our Kafka cluster has 3 brokers (`B1`, `B2`, `B3`). Our topic `orders` has 3 partitions (`P1`, `P2`, `P3`), and a replication factor of 2. This is how the partitions are distributed:
- `P1` lives on `B1` and `B3`
- `P2` lives on `B2` and `B3`
- `P3` lives on `B1` and `B2`

A consumer group with 3 consumers `C1`, `C2`, and `C3` joins. On joining,
- Kafka assigns `B2` as the Group Coordinator
- Kafka elects `C3` as the Group Leader 

The leader now gathers metadata from the consumers and proposes the following assignment to the Group Coordinator: 
- `C1` will read `P3` 
- `C2` will read `P1`
- `C3` will read `P2` 

The group Coordinator will approve this assignment and send each consumer their specific assignment.

##### Rebalancing
Any time a consumer joins or leaves, or the topicâ€™s partition count changes, Kafka triggers a rebalance. During rebalancing:
1. The Group Leader is reelected
2. Consumers pause fetching
3. New assignments are calculated and sent out

> Note: Kafka stores the Group Coordinator mapping for each consumer group in the `__consumer_offsets` topic. The group leader is an ephemeral position, dynamically created just for the purpose of assignment, and is not stored anywhere. 

### How Does a Kafka Consumer Connect?
When a Kafka consumer starts up:
1. It first connects to a bootstrap broker, just like a producer does, to discover the full cluster layout.
2. Using its group ID, the consumer learns which broker is the Group Coordinator for its consumer group.
3. It then joins the consumer group by talking to that Group Coordinator.
4. The Group Coordinator picks one of the consumers as the Group Leader, who gathers metadata and proposes a partition assignment.
5. The Group Coordinator finalizes the assignments and tells each consumer which partition(s) it should consume from.
6. The consumer then connects to the leader broker of its assigned partition(s) and starts consuming messages.

All of this coordination and communication is handled behind the scenes by the Kafka client library, so you typically donâ€™t need to implement it yourself.








